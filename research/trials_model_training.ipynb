{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5df151a0",
   "metadata": {},
   "source": [
    "# Environment Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "439fb644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspaces/TumorTracer'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Loading environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Changing directory to main directory for easy data access\n",
    "working_directory = os.getenv(\"WORKING_DIRECTORY\")\n",
    "os.chdir(working_directory)\n",
    "\n",
    "# Checking the change\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25a2b2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Git folder exists: True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Checking the change\n",
    "print(\"Git folder exists:\", Path(\".git\").exists())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e77ad8",
   "metadata": {},
   "source": [
    "# 3. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8554ae03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from cnnClassifier import get_logger\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "# Initializing the logger\n",
    "logger = get_logger()\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainingConfig:\n",
    "    \"\"\"\n",
    "    Immutable configuration class to store all parameters \n",
    "    and paths required for model training. \n",
    "    \"\"\"\n",
    "    root_dir: Path                                          # Directory for training artifacts\n",
    "    trained_model_path: Path                                # Final model output path\n",
    "    updated_base_model: Path                                # Pretrained model with custom head\n",
    "    training_data: Path                                     # Directory with training images\n",
    "    validation_data: Path                                   # Directory with validation images\n",
    "    params_augmentation: bool                               # Whether to apply augmentation\n",
    "    params_checkpoint: bool                                 # Whether created models need to be checkpointed\n",
    "    params_image_size: tuple[int, int, int]                 # Input image size, e.g., [224, 224, 3]\n",
    "    params_batch_size: int                                  # Batch size for training\n",
    "    params_epochs: int                                      # Total epochs\n",
    "    params_optimizer: str                                   # Optimizer to be used when recompling model\n",
    "    params_learning_rate: float                             # Learning rate for training\n",
    "    params_if_augmentation: Optional[Dict[str, Any]] = None # Dict of augmentation hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d3dd096",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnnClassifier.constants import CONFIG_FILE_PATH, PARAMS_FILE_PATH\n",
    "from cnnClassifier.utils.common import read_yaml, create_directories\n",
    "from cnnClassifier import get_logger\n",
    "\n",
    "# Initializing the logger\n",
    "logger = get_logger()\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self, config_file_path=CONFIG_FILE_PATH, params_file_path=PARAMS_FILE_PATH) -> None:\n",
    "        \"\"\"\n",
    "        Reads configuration files (config.yaml and params.yaml), \n",
    "        ensures necessary directories exist, and prepares structured config objects.\n",
    "\n",
    "        Args:\n",
    "        - config_file_path (str): Path to the config.yaml file.\n",
    "        - params_file_path (str): Path to the params.yaml file.\n",
    "        \"\"\"\n",
    "        # Validate and load config.yaml\n",
    "        if not Path(config_file_path).exists():\n",
    "            logger.error(f\"Config file not found at: {config_file_path}\")\n",
    "            raise FileNotFoundError(f\"Config file not found at: {config_file_path}\")\n",
    "        self.config = read_yaml(config_file_path)\n",
    "\n",
    "        # Validate and load params.yaml\n",
    "        if not Path(config_file_path).exists():\n",
    "            logger.error(f\"Params file not found at: {params_file_path}\")\n",
    "            raise FileNotFoundError(f\"Params file not found at: {params_file_path}\")\n",
    "        self.params = read_yaml(params_file_path)\n",
    "\n",
    "        logger.info(f\"Loading configuration from {config_file_path} and parameters from {params_file_path}\")\n",
    "\n",
    "        # Create the root artifacts directory (if not already present)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    def get_training_config(self) -> ModelTrainingConfig:\n",
    "        \"\"\"\n",
    "        Prepares and returns the ModelTrainingConfig object.\n",
    "\n",
    "        Returns:\n",
    "        - ModelTrainingConfig: Structured config for training the updated base model.\n",
    "        \"\"\"\n",
    "        training_config = self.config.model_training\n",
    "        training_params = self.params.model_training\n",
    "\n",
    "        # Ensure the data_ingestion directory exists\n",
    "        create_directories([training_config.root_dir])\n",
    "\n",
    "        # Load augmentation params only if augmentation is enabled and params for it are present\n",
    "        params_for_augmentation = {}\n",
    "        if training_params.AUGMENTATION and hasattr(training_params, \"AUGMENTATION_PARAMS\"):\n",
    "            params_for_augmentation = dict(training_params.AUGMENTATION_PARAMS )\n",
    "\n",
    "        training_config = ModelTrainingConfig(\n",
    "            root_dir=Path(training_config.root_dir),\n",
    "            trained_model_path=Path(training_config.trained_model_path),\n",
    "            updated_base_model=Path(training_config.updated_model_path),\n",
    "            training_data=Path(training_config.training_dataset),\n",
    "            validation_data=Path(training_config.validation_dataset),\n",
    "            params_augmentation=training_params.AUGMENTATION,\n",
    "            params_checkpoint=training_params.CHECKPOINT,\n",
    "            params_image_size=tuple(training_params.IMAGE_SIZE),\n",
    "            params_batch_size=training_params.BATCH_SIZE,\n",
    "            params_epochs=training_params.EPOCHS,\n",
    "            params_optimizer=training_params.OPTIMIZER,\n",
    "            params_learning_rate=training_params.LEARNING_RATE,\n",
    "            params_if_augmentation=params_for_augmentation,\n",
    "        )\n",
    "\n",
    "        logger.info(f\"ModelTrainingConfig created with: {training_config}\")\n",
    "\n",
    "        return training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89a4f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-05 14:21:18.696684: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-05 14:21:18.896073: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-05 14:21:19.969284: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-05 14:21:21.588455: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from math import ceil\n",
    "from typing import Union\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, DirectoryIterator\n",
    "from pathlib import Path\n",
    "from dataclasses import asdict\n",
    "from datetime import datetime\n",
    "\n",
    "from cnnClassifier.utils.common import create_directories, save_json\n",
    "from cnnClassifier import get_logger\n",
    "\n",
    "# Initializing the logger\n",
    "logger = get_logger()\n",
    "\n",
    "class ModelTraining:\n",
    "    \"\"\"\n",
    "    Initializes training pipeline with given configuration.\n",
    "\n",
    "    Core Responsibilities:\n",
    "    - Load a pre-defined base model from disk and recompile it with a fresh optimizer.\n",
    "    - Set up data generators for training and validation, with optional augmentation.\n",
    "    - Train the model across multiple epochs with optional checkpointing.\n",
    "    - Resume training from where it left off.\n",
    "    - Save class label mappings and model artifacts.\n",
    "\n",
    "    Public Methods:\n",
    "    - get_base_model(): Load and compile the pre-trained base model.\n",
    "    - get_data_generators(): Prepare train and validation data generators.\n",
    "    - train(): Train the model with checkpointing on best validation accuracy.\n",
    "    - resume_train(add_epochs): Continue training the model for additional epochs.\n",
    "    - save_class_indices(): Save class-to-index mapping as JSON for reproducibility.\n",
    "\n",
    "    Private Utilities:\n",
    "    - _build_generator(): Helper to construct data generators with standard settings.\n",
    "    - _create_checkpoint(): Creates a checkpoint directory and stores training metadata.\n",
    "    - _get_optimizer(): Returns optimizer based on config.\n",
    "    - _count_images_in_directory(): Utility to count image files recursively.\n",
    "    - _save_model(): Saves the model to disk at the given path.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: ModelTrainingConfig) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the model training pipeline.\n",
    "\n",
    "        - Sets random seeds for reproducibility.\n",
    "        - Prepares internal attributes for managing training, checkpoints, and model state.\n",
    "        \"\"\"\n",
    "        # Store configuration\n",
    "        self.config = config\n",
    "\n",
    "        # Set random seeds\n",
    "        seed = self.config.params_seed if hasattr(self.config, \"params_seed\") else 1234\n",
    "        tf.random.set_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "        # Initialize model and training attributes\n",
    "        self.output_model = None\n",
    "        self.training_generator = None\n",
    "        self.valid_generator = None\n",
    "        self.training_images = None\n",
    "        self.validation_images = None\n",
    "        self.last_epoch = 0\n",
    "        self.additional_epochs = 0\n",
    "        self.best_val_accuracy = 0\n",
    "\n",
    "        # Initialize checkpoint directory path with timestamp\n",
    "        curr_time = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "        self.checkpoint_path = Path(self.config.root_dir / f\"Checkpoint_{curr_time}\")\n",
    "\n",
    "\n",
    "    def get_base_model(self) -> None:\n",
    "        \"\"\"\n",
    "        Loads the base model form specified path.\n",
    "        \"\"\"\n",
    "        model_path = Path(self.config.updated_base_model)\n",
    "\n",
    "        if not model_path.exists():\n",
    "            logger.error(f\"Could not find model at {model_path}. Run the Base Model pipeline stage first.\")\n",
    "            raise FileNotFoundError(f\"Could not find model at {model_path}. Run the Base Model pipeline stage first.\")\n",
    "        \n",
    "        try:\n",
    "            self.output_model = tf.keras.models.load_model(model_path)\n",
    "            logger.info(f\"Successfully loaded the base model from {model_path}.\")\n",
    "\n",
    "            # Enabling Eager Execution (optional in TF 2.x) due to library requirement\n",
    "            tf.config.run_functions_eagerly(True)\n",
    "\n",
    "            # Recompile model with a fresh optimizer (required after loading)\n",
    "            self.output_model.compile(\n",
    "                optimizer=self._get_optimizer(),\n",
    "                loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                metrics=[\"accuracy\"]\n",
    "            )\n",
    "            logger.info(f\"Successfully recomplied the model.\")\n",
    "\n",
    "        except Exception as exception_error:\n",
    "            logger.error(f\"Unexpected error while loading the update base model at {model_path}: {exception_error}\")\n",
    "            raise \n",
    "            \n",
    "    \n",
    "    def get_data_generators(self) -> None:\n",
    "        \"\"\"\n",
    "        Update train and validation data generators using ImageDataGenerator.\n",
    "        Applies augmentation only on training data if enabled.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"Preparing ImageDataGenerators...\")\n",
    "\n",
    "            train_datagen = ImageDataGenerator(rescale=1.0/255, **self.config.params_if_augmentation)\n",
    "            valid_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "            self.training_generator = self._build_generator(train_datagen, self.config.training_data, \"Train\")\n",
    "            self.valid_generator = self._build_generator(valid_datagen, self.config.validation_data, \"Valid\")\n",
    "\n",
    "            # Ensure class-to-index mapping is consistent\n",
    "            if self.training_generator.class_indices != self.valid_generator.class_indices:\n",
    "                logger.error(\"Mismatch in class indices between train and validation generators!\")\n",
    "                raise ValueError(\"Mismatch in class indices between train and validation generators!\")\n",
    "\n",
    "            logger.info(\"ImageDataGenerators created successfully.\")\n",
    "        \n",
    "        except Exception as exception_error:\n",
    "            logger.error(f\"Unexpected error while creating data generators: {exception_error}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    def train(self) -> None:\n",
    "        \"\"\"\n",
    "        Trains the model using prepared generators.\n",
    "        \"\"\"\n",
    "        if self.output_model == None:\n",
    "            logger.error(\"Base model not found. Run get_base_model() before calling train().\")\n",
    "            raise ValueError(\"Base model not found. Run get_base_model() before calling train().\")\n",
    "        \n",
    "        if (self.config.params_checkpoint) and (not self.checkpoint_path.exists()) and (self.config.params_epochs >= 1):\n",
    "            self._create_checkpoint()\n",
    "        \n",
    "        try:\n",
    "            logger.info(\"Initializing model training...\")\n",
    "\n",
    "            # Counting the images in each of the datasets\n",
    "            self.training_images = self._count_images_in_directory(self.config.training_data)\n",
    "            self.validation_images = self._count_images_in_directory(self.config.validation_data)\n",
    "\n",
    "            # Fitting the model\n",
    "            for epoch in range(self.last_epoch+1, self.config.params_epochs + self.additional_epochs):\n",
    "                history = self.output_model.fit(\n",
    "                    self.training_generator,\n",
    "                    validation_data=self.valid_generator,\n",
    "                    initial_epoch=epoch,         # Sets starting point for correct logging\n",
    "                    epochs=epoch+1,              # Only running 1 epoch at a time\n",
    "                    steps_per_epoch=ceil(self.training_images / self.config.params_batch_size),\n",
    "                    validation_steps=ceil(self.validation_images / self.config.params_batch_size),\n",
    "                )\n",
    "\n",
    "                # Updating number of epochs completed\n",
    "                self.last_epoch = epoch + 1\n",
    "\n",
    "                if self.config.params_checkpoint:\n",
    "                    # Accessing accurary scores\n",
    "                    train_acc = history.history.get(\"accuracy\", [0])[0]\n",
    "                    val_acc = history.history.get(\"val_accuracy\", [0])[0]\n",
    "\n",
    "                    # If current model is better than prior best model, saving the model\n",
    "                    if val_acc > self.best_val_accuracy:\n",
    "                        self.best_val_accuracy = val_acc\n",
    "                        model_path = Path(self.checkpoint_path / f\"model_e{epoch+1:02d}_acc{train_acc:.4f}_vacc{val_acc:.4f}.h5\")\n",
    "                        self._save_model(save_path=model_path, model=self.output_model)\n",
    "                        logger.info(f\"Saved new best model at {model_path}\")\n",
    "\n",
    "            logger.info(\"Successfully trained model based on provided parameters.\")\n",
    "            self._save_model(save_path=self.config.trained_model_path, model=self.output_model)\n",
    "\n",
    "        except Exception as exception_error:\n",
    "            logger.error(f\"Unexpected error while training the model: {exception_error}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    def resume_train(self, add_epochs: int) -> None:\n",
    "        \"\"\"\n",
    "        Resumes model training for additional number of epochs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.additional_epochs == None:\n",
    "                self.additional_epochs = 0\n",
    "            self.additional_epochs += add_epochs\n",
    "\n",
    "            self.train()\n",
    "\n",
    "        except Exception as exception_error:\n",
    "            logger.error(f\"Unexpected error while resuming training: {exception_error}\")\n",
    "            raise\n",
    "    \n",
    "\n",
    "    def save_class_indices(self) -> None:\n",
    "        \"\"\"\n",
    "        Saves the class index mapping as a JSON file for future reference.\n",
    "        \"\"\"\n",
    "        if self.training_generator == None:\n",
    "            logger.error(\"Class indices not found. Run get_data_generators() before calling save_class_indices().\")\n",
    "            raise ValueError(\"Class indices not found. Run get_data_generators() before calling save_class_indices().\")\n",
    "\n",
    "        try:\n",
    "            save_path = Path(self.config.root_dir / \"class_indices.json\")\n",
    "            save_json(save_path=save_path, data=self.training_generator.class_indices)\n",
    "\n",
    "            if self.config.params_checkpoint and self.checkpoint_path.exists():\n",
    "                checkpoint_save_path = Path(self.checkpoint_path / \"class_indices.json\")\n",
    "                save_json(save_path=checkpoint_save_path, data=self.training_generator.class_indices)\n",
    "\n",
    "        except Exception as exception_error:\n",
    "            logger.error(f\"Unexpected error while saving class indices: {exception_error}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    def _build_generator(self, datagen: ImageDataGenerator, data_path: Union[str, Path], tag: str) -> DirectoryIterator:\n",
    "        \"\"\"\n",
    "        Helper to build a flow_from_directory generator with consistent options.\n",
    "\n",
    "        Args:\n",
    "        - datagen (ImageDataGenerator): Instance of the ImageDataGenerator.\n",
    "        - data_path (Union[str, Path]): Path to the directory containing images.\n",
    "        - tag (str): Label for logging context (\"Train\" or \"Valid\").\n",
    "\n",
    "        Returns:\n",
    "        - DirectoryIterator: Configured Keras generator for the given directory.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            data_path = Path(data_path)\n",
    "\n",
    "            if not data_path.exists():\n",
    "                logger.error(f\"{tag.title()} directory not found: {data_path}\")\n",
    "                raise FileNotFoundError(f\"{tag.title()} directory not found: {data_path}\")\n",
    "\n",
    "            # Building generator\n",
    "            generator_unit = datagen.flow_from_directory(\n",
    "                directory=data_path,\n",
    "                target_size=self.config.params_image_size[:2],\n",
    "                batch_size=self.config.params_batch_size,\n",
    "                class_mode=\"categorical\",\n",
    "                shuffle=True,\n",
    "            )\n",
    "\n",
    "            return generator_unit\n",
    "\n",
    "        except Exception as exception_error:\n",
    "            logger.error(f\"Unexpected error while build generator: {exception_error}\")\n",
    "            raise\n",
    "    \n",
    "\n",
    "    def _create_checkpoint(self) -> None:\n",
    "        \"\"\"\n",
    "        Creates a checkpoint directory and saves the training configuration as a JSON file.\n",
    "\n",
    "        Purpose:\n",
    "        - Ensures the checkpoint directory exists.\n",
    "        - Saves the current training configuration (hyperparameters) used in that run.\n",
    "        - Helps with reproducibility and traceability for saved models.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"Creating checkpoint directory...\")\n",
    "\n",
    "            save_path = Path(self.checkpoint_path / \"params_used.json\")\n",
    "            create_directories([self.checkpoint_path])\n",
    "\n",
    "            # Convert all Path objects to str recursively\n",
    "            config_dict = self._convert_paths_to_str(asdict(self.config))\n",
    "\n",
    "            save_json(save_path=save_path, data=config_dict)\n",
    "            \n",
    "            logger.info(f\"Checkpoint directory created.\")\n",
    "\n",
    "        except Exception as exception_error:\n",
    "            logger.error(f\"Unexpected error while creating checkpoint directroy: {exception_error}\")\n",
    "            raise \n",
    "\n",
    "\n",
    "    def _get_optimizer(self) -> tf.keras.optimizers.Optimizer:\n",
    "        \"\"\"\n",
    "        Dynamically selects and returns a TensorFlow optimizer based on the configuration.\n",
    "\n",
    "        Returns:\n",
    "            tf.keras.optimizers.Optimizer: Configured optimizer instance for model compilation.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Normalize optimizer name to lowercase for consistent matching\n",
    "            optimizer_name = self.config.params_optimizer.strip().upper()\n",
    "            optimizer = None\n",
    "\n",
    "            # Select optimizer based on configuration\n",
    "            if optimizer_name == \"SGD\":\n",
    "                optimizer = tf.keras.optimizers.SGD(learning_rate=self.config.params_learning_rate)\n",
    "\n",
    "            elif optimizer_name == \"RMSPROP\":\n",
    "                optimizer = tf.keras.optimizers.RMSprop(learning_rate=self.config.params_learning_rate) \n",
    "\n",
    "            else:\n",
    "                # Default to Adam if unsupported optimizer name is provided\n",
    "                if optimizer_name != \"ADAM\":\n",
    "                    logger.info(f\"Unsupported optimizer name {optimizer_name} provided. Falling back to 'Adam'.\")\n",
    "                    optimizer_name = \"ADAM\"\n",
    "\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate=self.config.params_learning_rate)\n",
    "\n",
    "            logger.info(f\"Optimizer '{optimizer_name}' initialized and returned.\")\n",
    "            return optimizer\n",
    "        \n",
    "        except Exception as exception_error:\n",
    "            logger.error(f\"Unexpected error while loading optimizer: {exception_error}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _count_images_in_directory(directory_path: Union[str, Path]) -> int:\n",
    "        \"\"\"\n",
    "        Counts the total number of image files in a directory and its subfolders.\n",
    "\n",
    "        Args:\n",
    "        - directory_path (str or Path): Path to the dataset root (e.g., train or valid)\n",
    "\n",
    "        Returns:\n",
    "        - int: Total number of images found\n",
    "        \"\"\"\n",
    "        try:\n",
    "            directory_path = Path(directory_path)\n",
    "            total_images = 0\n",
    "\n",
    "            if not directory_path.exists():\n",
    "                logger.error(f\"Could not find the path {directory_path}\")\n",
    "                raise FileNotFoundError(f\"Could not find the path {directory_path}\")\n",
    "            \n",
    "            for _, _, files in os.walk(directory_path):\n",
    "                total_images += len([f for f in files if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))])\n",
    "\n",
    "            if total_images == 0:\n",
    "                logger.error(f\"No images found in {directory_path}\")\n",
    "                raise ValueError(f\"No images found in {directory_path}\")       \n",
    "\n",
    "            return total_images\n",
    "\n",
    "        except Exception as exception_error:\n",
    "            logger.error(f\"Unexpected error while counting images in directory: {exception_error}\")\n",
    "            raise\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def _convert_paths_to_str(obj: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Recursively convert Path objects in a nested dictionary to strings.\n",
    "        \"\"\"\n",
    "        output = {}\n",
    "        for key, value in obj.items():\n",
    "            if isinstance(value, Path):\n",
    "                output[key] = str(value)\n",
    "            elif isinstance(value, dict):\n",
    "                output[key] = ModelTraining._convert_paths_to_str(value)\n",
    "            else:\n",
    "                output[key] = value\n",
    "        return output\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _save_model(save_path: Path, model: tf.keras.Model) -> None:\n",
    "        \"\"\"\n",
    "        Saves a given model to the specified path.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            create_directories([save_path.parent])\n",
    "            model.save(save_path)\n",
    "            logger.info(f\"Model saved at: {save_path}\")\n",
    "        \n",
    "        except Exception as exception_error:\n",
    "            logger.error(f\"Unexpected error while saving the model at {save_path}: {exception_error}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09059d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-05 14:21:25,388: INFO: common: YAML file: config/config.yaml loaded successfully]\n",
      "[2025-07-05 14:21:25,397: INFO: common: YAML file: params/params.yaml loaded successfully]\n",
      "[2025-07-05 14:21:25,398: INFO: 377247663: Loading configuration from config/config.yaml and parameters from params/params.yaml]\n",
      "[2025-07-05 14:21:25,402: INFO: common: Directory: artifacts created successfully.]\n",
      "[2025-07-05 14:21:25,403: INFO: common: Directory: artifacts/model_training created successfully.]\n",
      "[2025-07-05 14:21:25,404: INFO: 377247663: ModelTrainingConfig created with: ModelTrainingConfig(root_dir=PosixPath('artifacts/model_training'), trained_model_path=PosixPath('artifacts/model_training/trained_model.h5'), updated_base_model=PosixPath('artifacts/base_model/updated_base_model.h5'), training_data=PosixPath('artifacts/data_ingestion/Data/train'), validation_data=PosixPath('artifacts/data_ingestion/Data/valid'), params_augmentation=True, params_checkpoint=True, params_image_size=(224, 224, 3), params_batch_size=16, params_epochs=1, params_optimizer='Adam', params_learning_rate=0.01, params_if_augmentation={'rotation_range': 15, 'width_shift_range': 0.1, 'height_shift_range': 0.1, 'shear_range': 0.1, 'zoom_range': 0.2, 'horizontal_flip': True, 'fill_mode': 'nearest'})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-05 14:21:25,929: INFO: 828250690: Successfully loaded the base model from artifacts/base_model/updated_base_model.h5.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Successfully loaded the base model from artifacts/base_model/updated_base_model.h5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-05 14:21:25,933: INFO: 828250690: Optimizer 'ADAM' initialized and returned.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Optimizer 'ADAM' initialized and returned.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-05 14:21:25,936: INFO: 828250690: Successfully recomplied the model.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Successfully recomplied the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-05 14:21:25,937: INFO: 828250690: Preparing ImageDataGenerators...]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Preparing ImageDataGenerators...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 613 images belonging to 4 classes.\n",
      "Found 72 images belonging to 4 classes.\n",
      "[2025-07-05 14:21:25,962: INFO: 828250690: ImageDataGenerators created successfully.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:ImageDataGenerators created successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-05 14:21:25,964: INFO: 828250690: Creating checkpoint directory...]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Creating checkpoint directory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-05 14:21:25,966: INFO: common: Directory: artifacts/model_training/Checkpoint_20250705_1421 created successfully.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_test:Directory: artifacts/model_training/Checkpoint_20250705_1421 created successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-05 14:21:25,968: INFO: common: Directory: artifacts/model_training/Checkpoint_20250705_1421 created successfully.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_test:Directory: artifacts/model_training/Checkpoint_20250705_1421 created successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-05 14:21:25,970: ERROR: common: Failed to seialize data to JSON at artifacts/model_training/Checkpoint_20250705_1421/params_used.json: Object of type PosixPath is not JSON serializable]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:cnnClassifierLogger_test:Failed to seialize data to JSON at artifacts/model_training/Checkpoint_20250705_1421/params_used.json: Object of type PosixPath is not JSON serializable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-05 14:21:25,971: ERROR: 828250690: Unexpected error while creating checkpoint directroy: Object of type PosixPath is not JSON serializable]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:cnnClassifierLogger_running:Unexpected error while creating checkpoint directroy: Object of type PosixPath is not JSON serializable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-05 14:21:25,972: ERROR: 1222839666: Unexpected error during model training pipeline: Object of type PosixPath is not JSON serializable]\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_28161/1222839666.py\", line 8, in <module>\n",
      "    training_constructor.train()\n",
      "  File \"/tmp/ipykernel_28161/828250690.py\", line 139, in train\n",
      "    self._create_checkpoint()\n",
      "  File \"/tmp/ipykernel_28161/828250690.py\", line 268, in _create_checkpoint\n",
      "    save_json(save_path=save_path, data=asdict(self.config))\n",
      "  File \"/workspaces/TumorTracer/src/cnnClassifier/utils/common.py\", line 134, in save_json\n",
      "    raise exception_error\n",
      "  File \"/workspaces/TumorTracer/src/cnnClassifier/utils/common.py\", line 127, in save_json\n",
      "    json.dump(data, file, indent=4)\n",
      "  File \"/home/codespace/.python/current/lib/python3.12/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/codespace/.python/current/lib/python3.12/json/encoder.py\", line 432, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/codespace/.python/current/lib/python3.12/json/encoder.py\", line 406, in _iterencode_dict\n",
      "    yield from chunks\n",
      "  File \"/home/codespace/.python/current/lib/python3.12/json/encoder.py\", line 439, in _iterencode\n",
      "    o = _default(o)\n",
      "        ^^^^^^^^^^^\n",
      "  File \"/home/codespace/.python/current/lib/python3.12/json/encoder.py\", line 180, in default\n",
      "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
      "TypeError: Object of type PosixPath is not JSON serializable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:cnnClassifierLogger_running:Unexpected error during model training pipeline: Object of type PosixPath is not JSON serializable\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_28161/1222839666.py\", line 8, in <module>\n",
      "    training_constructor.train()\n",
      "  File \"/tmp/ipykernel_28161/828250690.py\", line 139, in train\n",
      "    self._create_checkpoint()\n",
      "  File \"/tmp/ipykernel_28161/828250690.py\", line 268, in _create_checkpoint\n",
      "    save_json(save_path=save_path, data=asdict(self.config))\n",
      "  File \"/workspaces/TumorTracer/src/cnnClassifier/utils/common.py\", line 134, in save_json\n",
      "    raise exception_error\n",
      "  File \"/workspaces/TumorTracer/src/cnnClassifier/utils/common.py\", line 127, in save_json\n",
      "    json.dump(data, file, indent=4)\n",
      "  File \"/home/codespace/.python/current/lib/python3.12/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/home/codespace/.python/current/lib/python3.12/json/encoder.py\", line 432, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/codespace/.python/current/lib/python3.12/json/encoder.py\", line 406, in _iterencode_dict\n",
      "    yield from chunks\n",
      "  File \"/home/codespace/.python/current/lib/python3.12/json/encoder.py\", line 439, in _iterencode\n",
      "    o = _default(o)\n",
      "        ^^^^^^^^^^^\n",
      "  File \"/home/codespace/.python/current/lib/python3.12/json/encoder.py\", line 180, in default\n",
      "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
      "TypeError: Object of type PosixPath is not JSON serializable\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type PosixPath is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m training_constructor.get_base_model()\n\u001b[32m      7\u001b[39m training_constructor.get_data_generators()\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mtraining_constructor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m training_constructor.save_class_indices()\n\u001b[32m     10\u001b[39m training_constructor.resume_train(add_epochs=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 139\u001b[39m, in \u001b[36mModelTraining.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mBase model not found. Run get_base_model() before calling train().\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.config.params_checkpoint) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.checkpoint_path.exists()) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.config.params_epochs >= \u001b[32m1\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    142\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mInitializing model training...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 268\u001b[39m, in \u001b[36mModelTraining._create_checkpoint\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    266\u001b[39m     save_path = Path(\u001b[38;5;28mself\u001b[39m.checkpoint_path / \u001b[33m\"\u001b[39m\u001b[33mparams_used.json\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    267\u001b[39m     create_directories([\u001b[38;5;28mself\u001b[39m.checkpoint_path])\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m     \u001b[43msave_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43masdict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCheckpoint directory created.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/TumorTracer/src/cnnClassifier/utils/common.py:134\u001b[39m, in \u001b[36msave_json\u001b[39m\u001b[34m(save_path, data)\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception_error:\n\u001b[32m    132\u001b[39m     \u001b[38;5;66;03m# Raised when the data can't be serialized (e.g., a set, custom object, or a NumPy array without conversion).\u001b[39;00m\n\u001b[32m    133\u001b[39m     logger.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to seialize data to JSON at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception_error\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_error\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception_error:\n\u001b[32m    137\u001b[39m     \u001b[38;5;66;03m# Raised if the path is invalid (rare here because we’re creating it, but still a good fallback).\u001b[39;00m\n\u001b[32m    138\u001b[39m     logger.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDirectory not found for saving JSON at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path.parent\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/TumorTracer/src/cnnClassifier/utils/common.py:127\u001b[39m, in \u001b[36msave_json\u001b[39m\u001b[34m(save_path, data)\u001b[39m\n\u001b[32m    125\u001b[39m     \u001b[38;5;66;03m# Write JSON to file\u001b[39;00m\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(save_path, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m         \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mJSON file saved at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception_error:\n\u001b[32m    132\u001b[39m     \u001b[38;5;66;03m# Raised when the data can't be serialized (e.g., a set, custom object, or a NumPy array without conversion).\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/json/__init__.py:179\u001b[39m, in \u001b[36mdump\u001b[39m\u001b[34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[39m\n\u001b[32m    173\u001b[39m     iterable = \u001b[38;5;28mcls\u001b[39m(skipkeys=skipkeys, ensure_ascii=ensure_ascii,\n\u001b[32m    174\u001b[39m         check_circular=check_circular, allow_nan=allow_nan, indent=indent,\n\u001b[32m    175\u001b[39m         separators=separators,\n\u001b[32m    176\u001b[39m         default=default, sort_keys=sort_keys, **kw).iterencode(obj)\n\u001b[32m    177\u001b[39m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/json/encoder.py:432\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode\u001b[39m\u001b[34m(o, _current_indent_level)\u001b[39m\n\u001b[32m    430\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    434\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/json/encoder.py:406\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode_dict\u001b[39m\u001b[34m(dct, _current_indent_level)\u001b[39m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    405\u001b[39m             chunks = _iterencode(value, _current_indent_level)\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    408\u001b[39m     _current_indent_level -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/json/encoder.py:439\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode\u001b[39m\u001b[34m(o, _current_indent_level)\u001b[39m\n\u001b[32m    437\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCircular reference detected\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    438\u001b[39m     markers[markerid] = o\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m o = \u001b[43m_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/json/encoder.py:180\u001b[39m, in \u001b[36mJSONEncoder.default\u001b[39m\u001b[34m(self, o)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[32m    162\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[33;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[33;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    178\u001b[39m \n\u001b[32m    179\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    181\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mis not JSON serializable\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Object of type PosixPath is not JSON serializable"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config_manager = ConfigurationManager()\n",
    "    training_config = config_manager.get_training_config()\n",
    "\n",
    "    training_constructor = ModelTraining(config=training_config)\n",
    "    training_constructor.get_base_model()\n",
    "    training_constructor.get_data_generators()\n",
    "    training_constructor.train()\n",
    "    training_constructor.save_class_indices()\n",
    "    training_constructor.resume_train(add_epochs=1)\n",
    "\n",
    "except Exception as exception_error:\n",
    "    logger.exception(f\"Unexpected error during model training pipeline: {exception_error}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3066b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
