{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5df151a0",
   "metadata": {},
   "source": [
    "# Environment Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "439fb644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspaces/TumorTracer'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Loading environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Changing directory to main directory for easy data access\n",
    "working_directory = os.getenv(\"WORKING_DIRECTORY\")\n",
    "os.chdir(working_directory)\n",
    "\n",
    "# Checking the change\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25a2b2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Git folder exists: True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Checking the change\n",
    "print(\"Git folder exists:\", Path(\".git\").exists())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e77ad8",
   "metadata": {},
   "source": [
    "# 3. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8554ae03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from cnnClassifier import get_logger\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "# Initializing the logger\n",
    "logger = get_logger()\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainingConfig:\n",
    "    \"\"\"\n",
    "    Immutable configuration class to store all parameters \n",
    "    and paths required for model training. \n",
    "    \"\"\"\n",
    "    root_dir: Path                                          # Directory for training artifacts\n",
    "    trained_model_path: Path                                # Final model output path\n",
    "    updated_base_model: Path                                # Pretrained model with custom head\n",
    "    training_data: Path                                     # Directory with training images\n",
    "    validation_data: Path                                   # Directory with validation images\n",
    "    params_augmentation: bool                               # Whether to apply augmentation\n",
    "    params_checkpoint: bool                                 # Whether created models need to be checkpointed\n",
    "    params_mlflow: bool                                     # Whether models need to be tracker in mlflow\n",
    "    params_image_size: tuple[int, int, int]                 # Input image size, e.g., [224, 224, 3]\n",
    "    params_batch_size: int                                  # Batch size for training\n",
    "    params_epochs: int                                      # Total epochs\n",
    "    params_optimizer: str                                   # Optimizer to be used when recompling model\n",
    "    params_learning_rate: float                             # Learning rate for training\n",
    "    params_if_augmentation: Optional[Dict[str, Any]] = None # Dict of augmentation hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d3dd096",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-07 07:15:02.061814: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-07 07:15:07.415268: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-07 07:15:19.133136: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-07 07:15:30.083680: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from cnnClassifier.constants import CONFIG_FILE_PATH, PARAMS_FILE_PATH\n",
    "from cnnClassifier.utils.common import read_yaml, create_directories\n",
    "from cnnClassifier import get_logger\n",
    "\n",
    "# Initializing the logger\n",
    "logger = get_logger()\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self, config_file_path=CONFIG_FILE_PATH, params_file_path=PARAMS_FILE_PATH) -> None:\n",
    "        \"\"\"\n",
    "        Reads configuration files (config.yaml and params.yaml), \n",
    "        ensures necessary directories exist, and prepares structured config objects.\n",
    "\n",
    "        Args:\n",
    "        - config_file_path (str): Path to the config.yaml file.\n",
    "        - params_file_path (str): Path to the params.yaml file.\n",
    "        \"\"\"\n",
    "        # Validate and load config.yaml\n",
    "        if not Path(config_file_path).exists():\n",
    "            logger.error(f\"Config file not found at: {config_file_path}\")\n",
    "            raise FileNotFoundError(f\"Config file not found at: {config_file_path}\")\n",
    "        self.config = read_yaml(config_file_path)\n",
    "\n",
    "        # Validate and load params.yaml\n",
    "        if not Path(config_file_path).exists():\n",
    "            logger.error(f\"Params file not found at: {params_file_path}\")\n",
    "            raise FileNotFoundError(f\"Params file not found at: {params_file_path}\")\n",
    "        self.params = read_yaml(params_file_path)\n",
    "\n",
    "        logger.info(f\"Loading configuration from {config_file_path} and parameters from {params_file_path}\")\n",
    "\n",
    "        # Create the root artifacts directory (if not already present)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    def get_training_config(self) -> ModelTrainingConfig:\n",
    "        \"\"\"\n",
    "        Prepares and returns the ModelTrainingConfig object.\n",
    "\n",
    "        Returns:\n",
    "        - ModelTrainingConfig: Structured config for training the updated base model.\n",
    "        \"\"\"\n",
    "        training_config = self.config.model_training\n",
    "        training_params = self.params.model_training\n",
    "\n",
    "        # Ensure the data_ingestion directory exists\n",
    "        create_directories([training_config.root_dir])\n",
    "\n",
    "        # Load augmentation params only if augmentation is enabled and params for it are present\n",
    "        params_for_augmentation = {}\n",
    "        if training_params.AUGMENTATION and hasattr(training_params, \"AUGMENTATION_PARAMS\"):\n",
    "            params_for_augmentation = dict(training_params.AUGMENTATION_PARAMS )\n",
    "\n",
    "        training_config = ModelTrainingConfig(\n",
    "            root_dir=Path(training_config.root_dir),\n",
    "            trained_model_path=Path(training_config.trained_model_path),\n",
    "            updated_base_model=Path(training_config.updated_model_path),\n",
    "            training_data=Path(training_config.training_dataset),\n",
    "            validation_data=Path(training_config.validation_dataset),\n",
    "            params_augmentation=training_params.AUGMENTATION,\n",
    "            params_checkpoint=training_params.CHECKPOINT,\n",
    "            params_mlflow=training_params.MLFLOW_TRACKING,\n",
    "            params_image_size=tuple(training_params.IMAGE_SIZE),\n",
    "            params_batch_size=training_params.BATCH_SIZE,\n",
    "            params_epochs=training_params.EPOCHS,\n",
    "            params_optimizer=training_params.OPTIMIZER,\n",
    "            params_learning_rate=training_params.LEARNING_RATE,\n",
    "            params_if_augmentation=params_for_augmentation,\n",
    "        )\n",
    "\n",
    "        logger.info(f\"ModelTrainingConfig created with: {training_config}\")\n",
    "\n",
    "        return training_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81607de",
   "metadata": {},
   "source": [
    "## Version 1 - Manually created callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89a4f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from math import ceil\n",
    "from typing import Union\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, DirectoryIterator # type: ignore\n",
    "from pathlib import Path\n",
    "from dataclasses import asdict\n",
    "from datetime import datetime\n",
    "\n",
    "from cnnClassifier.utils.common import create_directories, save_json\n",
    "from cnnClassifier import get_logger\n",
    "\n",
    "# Initializing the logger\n",
    "logger = get_logger()\n",
    "\n",
    "class ModelTraining:\n",
    "    \"\"\"\n",
    "    Initializes training pipeline with given configuration.\n",
    "\n",
    "    Core Responsibilities:\n",
    "    - Load a pre-defined base model from disk and recompile it with a fresh optimizer.\n",
    "    - Set up data generators for training and validation, with optional augmentation.\n",
    "    - Train the model across multiple epochs with optional checkpointing.\n",
    "    - Resume training from where it left off.\n",
    "    - Save class label mappings and model artifacts.\n",
    "\n",
    "    Public Methods:\n",
    "    - get_base_model(): Load and compile the pre-trained base model.\n",
    "    - get_data_generators(): Prepare train and validation data generators.\n",
    "    - train(): Train the model with checkpointing on best validation accuracy.\n",
    "    - resume_train(add_epochs): Continue training the model for additional epochs.\n",
    "    - save_class_indices(): Save class-to-index mapping as JSON for reproducibility.\n",
    "\n",
    "    Private Utilities:\n",
    "    - _build_generator(): Helper to construct data generators with standard settings.\n",
    "    - _create_checkpoint(): Creates a checkpoint directory and stores training metadata.\n",
    "    - _get_optimizer(): Returns optimizer based on config.\n",
    "    - _count_images_in_directory(): Utility to count image files recursively.\n",
    "    - _save_model(): Saves the model to disk at the given path.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: ModelTrainingConfig) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the model training pipeline.\n",
    "\n",
    "        - Sets random seeds for reproducibility.\n",
    "        - Prepares internal attributes for managing training, checkpoints, and model state.\n",
    "        \"\"\"\n",
    "        # Store configuration\n",
    "        self.config = config\n",
    "\n",
    "        # Set random seeds\n",
    "        seed = self.config.params_seed if hasattr(self.config, \"params_seed\") else 1234\n",
    "        tf.random.set_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "        # Initialize model and training attributes\n",
    "        self.output_model = None\n",
    "        self.training_generator = None\n",
    "        self.valid_generator = None\n",
    "        self.training_images = None\n",
    "        self.validation_images = None\n",
    "        self.last_epoch = 0\n",
    "        self.additional_epochs = 0\n",
    "        self.best_val_accuracy = 0\n",
    "\n",
    "        # Initialize checkpoint directory path with timestamp\n",
    "        curr_time = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "        self.checkpoint_path = Path(self.config.root_dir / f\"Checkpoint_{curr_time}\")\n",
    "\n",
    "\n",
    "    def get_base_model(self) -> None:\n",
    "        \"\"\"\n",
    "        Loads the base model form specified path.\n",
    "        \"\"\"\n",
    "        model_path = Path(self.config.updated_base_model)\n",
    "\n",
    "        if not model_path.exists():\n",
    "            logger.error(f\"Could not find model at {model_path}. Run the Base Model pipeline stage first.\")\n",
    "            raise FileNotFoundError(f\"Could not find model at {model_path}. Run the Base Model pipeline stage first.\")\n",
    "        \n",
    "        try:\n",
    "            self.output_model = tf.keras.models.load_model(model_path)\n",
    "            logger.info(f\"Successfully loaded the base model from {model_path}.\")\n",
    "\n",
    "            # Enabling Eager Execution (optional in TF 2.x) due to library requirement\n",
    "            tf.config.run_functions_eagerly(True)\n",
    "\n",
    "            # Recompile model with a fresh optimizer (required after loading)\n",
    "            self.output_model.compile(\n",
    "                optimizer=self._get_optimizer(),\n",
    "                loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                metrics=[\"accuracy\"]\n",
    "            )\n",
    "            logger.info(f\"Successfully recomplied the model.\")\n",
    "\n",
    "        except Exception as exception_error:\n",
    "            logger.error(f\"Unexpected error while loading the update base model at {model_path}: {exception_error}\")\n",
    "            raise \n",
    "            \n",
    "    \n",
    "    def get_data_generators(self) -> None:\n",
    "        \"\"\"\n",
    "        Update train and validation data generators using ImageDataGenerator.\n",
    "        Applies augmentation only on training data if enabled.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"Preparing ImageDataGenerators...\")\n",
    "\n",
    "            train_datagen = ImageDataGenerator(rescale=1.0/255, **self.config.params_if_augmentation)\n",
    "            valid_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "            self.training_generator = self._build_generator(train_datagen, self.config.training_data, \"Train\")\n",
    "            self.valid_generator = self._build_generator(valid_datagen, self.config.validation_data, \"Valid\")\n",
    "\n",
    "            # Ensure class-to-index mapping is consistent\n",
    "            if self.training_generator.class_indices != self.valid_generator.class_indices:\n",
    "                logger.error(\"Mismatch in class indices between train and validation generators!\")\n",
    "                raise ValueError(\"Mismatch in class indices between train and validation generators!\")\n",
    "\n",
    "            logger.info(\"ImageDataGenerators created successfully.\")\n",
    "        \n",
    "        except Exception as exception_error:\n",
    "            logger.error(f\"Unexpected error while creating data generators: {exception_error}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    def train(self) -> None:\n",
    "        \"\"\"\n",
    "        Trains the model using prepared generators.\n",
    "        \"\"\"\n",
    "        if self.output_model == None:\n",
    "            logger.error(\"Base model not found. Run get_base_model() before calling train().\")\n",
    "            raise ValueError(\"Base model not found. Run get_base_model() before calling train().\")\n",
    "        \n",
    "        if (self.config.params_checkpoint) and (not self.checkpoint_path.exists()) and (self.config.params_epochs >= 1):\n",
    "            self._create_checkpoint()\n",
    "        \n",
    "        try:\n",
    "            logger.info(\"Initializing model training...\")\n",
    "\n",
    "            # Counting the images in each of the datasets\n",
    "            self.training_images = self._count_images_in_directory(self.config.training_data)\n",
    "            self.validation_images = self._count_images_in_directory(self.config.validation_data)\n",
    "            \n",
    "            # Fitting the model\n",
    "            for epoch in range(self.last_epoch, self.config.params_epochs + self.additional_epochs):\n",
    "                history = self.output_model.fit(\n",
    "                    self.training_generator,\n",
    "                    validation_data=self.valid_generator,\n",
    "                    initial_epoch=epoch,         # Sets starting point for correct logging\n",
    "                    epochs=epoch+1,              # Only running 1 epoch at a time\n",
    "                    steps_per_epoch=ceil(self.training_images / self.config.params_batch_size),\n",
    "                    validation_steps=ceil(self.validation_images / self.config.params_batch_size),\n",
    "                )\n",
    "\n",
    "                # Updating number of epochs completed\n",
    "                self.last_epoch = epoch\n",
    "\n",
    "                if self.config.params_checkpoint:\n",
    "                    # Accessing accurary scores\n",
    "                    train_acc = history.history.get(\"accuracy\", [0])[0]\n",
    "                    val_acc = history.history.get(\"val_accuracy\", [0])[0]\n",
    "\n",
    "                    # If current model is better than prior best model, saving the model\n",
    "                    if val_acc > self.best_val_accuracy:\n",
    "                        self.best_val_accuracy = val_acc\n",
    "                        model_path = Path(self.checkpoint_path / f\"model_e{epoch+1:02d}_acc{train_acc:.4f}_vacc{val_acc:.4f}.h5\")\n",
    "                        self._save_model(save_path=model_path, model=self.output_model)\n",
    "                        logger.info(f\"Saved new best model at {model_path}\")\n",
    "\n",
    "            logger.info(\"Successfully trained model based on provided parameters.\")\n",
    "            self._save_model(save_path=self.config.trained_model_path, model=self.output_model)\n",
    "\n",
    "        except Exception as exception_error:\n",
    "            logger.error(f\"Unexpected error while training the model: {exception_error}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    def resume_train(self, add_epochs: int) -> None:\n",
    "        \"\"\"\n",
    "        Resumes model training for additional number of epochs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.additional_epochs == None:\n",
    "                self.additional_epochs = 0\n",
    "            self.additional_epochs += add_epochs\n",
    "\n",
    "            self.train()\n",
    "\n",
    "        except Exception as exception_error:\n",
    "            logger.error(f\"Unexpected error while resuming training: {exception_error}\")\n",
    "            raise\n",
    "    \n",
    "\n",
    "    def save_class_indices(self) -> None:\n",
    "        \"\"\"\n",
    "        Saves the class index mapping as a JSON file for future reference.\n",
    "        \"\"\"\n",
    "        if self.training_generator == None:\n",
    "            logger.error(\"Class indices not found. Run get_data_generators() before calling save_class_indices().\")\n",
    "            raise ValueError(\"Class indices not found. Run get_data_generators() before calling save_class_indices().\")\n",
    "\n",
    "        try:\n",
    "            save_path = Path(self.config.root_dir / \"class_indices.json\")\n",
    "            save_json(save_path=save_path, data=self.training_generator.class_indices)\n",
    "\n",
    "            if self.config.params_checkpoint and self.checkpoint_path.exists():\n",
    "                checkpoint_save_path = Path(self.checkpoint_path / \"class_indices.json\")\n",
    "                save_json(save_path=checkpoint_save_path, data=self.training_generator.class_indices)\n",
    "\n",
    "        except Exception as exception_error:\n",
    "            logger.error(f\"Unexpected error while saving class indices: {exception_error}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    def _build_generator(self, datagen: ImageDataGenerator, data_path: Union[str, Path], tag: str) -> DirectoryIterator:\n",
    "        \"\"\"\n",
    "        Helper to build a flow_from_directory generator with consistent options.\n",
    "\n",
    "        Args:\n",
    "        - datagen (ImageDataGenerator): Instance of the ImageDataGenerator.\n",
    "        - data_path (Union[str, Path]): Path to the directory containing images.\n",
    "        - tag (str): Label for logging context (\"Train\" or \"Valid\").\n",
    "\n",
    "        Returns:\n",
    "        - DirectoryIterator: Configured Keras generator for the given directory.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            data_path = Path(data_path)\n",
    "\n",
    "            if not data_path.exists():\n",
    "                logger.error(f\"{tag.title()} directory not found: {data_path}\")\n",
    "                raise FileNotFoundError(f\"{tag.title()} directory not found: {data_path}\")\n",
    "\n",
    "            # Building generator\n",
    "            generator_unit = datagen.flow_from_directory(\n",
    "                directory=data_path,\n",
    "                target_size=self.config.params_image_size[:2],\n",
    "                batch_size=self.config.params_batch_size,\n",
    "                class_mode=\"categorical\",\n",
    "                shuffle=True,\n",
    "            )\n",
    "\n",
    "            return generator_unit\n",
    "\n",
    "        except Exception as exception_error:\n",
    "            logger.error(f\"Unexpected error while build generator: {exception_error}\")\n",
    "            raise\n",
    "    \n",
    "\n",
    "    def _create_checkpoint(self) -> None:\n",
    "        \"\"\"\n",
    "        Creates a checkpoint directory and saves the training configuration as a JSON file.\n",
    "\n",
    "        Purpose:\n",
    "        - Ensures the checkpoint directory exists.\n",
    "        - Saves the current training configuration (hyperparameters) used in that run.\n",
    "        - Helps with reproducibility and traceability for saved models.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"Creating checkpoint directory...\")\n",
    "\n",
    "            save_path = Path(self.checkpoint_path / \"params_used.json\")\n",
    "            create_directories([self.checkpoint_path])\n",
    "\n",
    "            # Convert all Path objects to str recursively\n",
    "            config_dict = self._convert_paths_to_str(asdict(self.config))\n",
    "\n",
    "            save_json(save_path=save_path, data=config_dict)\n",
    "            \n",
    "            logger.info(f\"Checkpoint directory created.\")\n",
    "\n",
    "        except Exception as exception_error:\n",
    "            logger.error(f\"Unexpected error while creating checkpoint directroy: {exception_error}\")\n",
    "            raise \n",
    "\n",
    "\n",
    "    def _get_optimizer(self) -> tf.keras.optimizers.Optimizer:\n",
    "        \"\"\"\n",
    "        Dynamically selects and returns a TensorFlow optimizer based on the configuration.\n",
    "\n",
    "        Returns:\n",
    "            tf.keras.optimizers.Optimizer: Configured optimizer instance for model compilation.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Normalize optimizer name to lowercase for consistent matching\n",
    "            optimizer_name = self.config.params_optimizer.strip().upper()\n",
    "            optimizer = None\n",
    "\n",
    "            # Select optimizer based on configuration\n",
    "            if optimizer_name == \"SGD\":\n",
    "                optimizer = tf.keras.optimizers.SGD(learning_rate=self.config.params_learning_rate)\n",
    "\n",
    "            elif optimizer_name == \"RMSPROP\":\n",
    "                optimizer = tf.keras.optimizers.RMSprop(learning_rate=self.config.params_learning_rate) \n",
    "\n",
    "            else:\n",
    "                # Default to Adam if unsupported optimizer name is provided\n",
    "                if optimizer_name != \"ADAM\":\n",
    "                    logger.info(f\"Unsupported optimizer name {optimizer_name} provided. Falling back to 'Adam'.\")\n",
    "                    optimizer_name = \"ADAM\"\n",
    "\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate=self.config.params_learning_rate)\n",
    "\n",
    "            logger.info(f\"Optimizer '{optimizer_name}' initialized and returned.\")\n",
    "            return optimizer\n",
    "        \n",
    "        except Exception as exception_error:\n",
    "            logger.error(f\"Unexpected error while loading optimizer: {exception_error}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _count_images_in_directory(directory_path: Union[str, Path]) -> int:\n",
    "        \"\"\"\n",
    "        Counts the total number of image files in a directory and its subfolders.\n",
    "\n",
    "        Args:\n",
    "        - directory_path (str or Path): Path to the dataset root (e.g., train or valid)\n",
    "\n",
    "        Returns:\n",
    "        - int: Total number of images found\n",
    "        \"\"\"\n",
    "        try:\n",
    "            directory_path = Path(directory_path)\n",
    "            total_images = 0\n",
    "\n",
    "            if not directory_path.exists():\n",
    "                logger.error(f\"Could not find the path {directory_path}\")\n",
    "                raise FileNotFoundError(f\"Could not find the path {directory_path}\")\n",
    "            \n",
    "            for _, _, files in os.walk(directory_path):\n",
    "                total_images += len([f for f in files if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))])\n",
    "\n",
    "            if total_images == 0:\n",
    "                logger.error(f\"No images found in {directory_path}\")\n",
    "                raise ValueError(f\"No images found in {directory_path}\")       \n",
    "\n",
    "            return total_images\n",
    "\n",
    "        except Exception as exception_error:\n",
    "            logger.error(f\"Unexpected error while counting images in directory: {exception_error}\")\n",
    "            raise\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def _convert_paths_to_str(obj: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Recursively convert Path objects in a nested dictionary to strings.\n",
    "        \"\"\"\n",
    "        output = {}\n",
    "        for key, value in obj.items():\n",
    "            if isinstance(value, Path):\n",
    "                output[key] = str(value)\n",
    "            elif isinstance(value, dict):\n",
    "                output[key] = ModelTraining._convert_paths_to_str(value)\n",
    "            else:\n",
    "                output[key] = value\n",
    "        return output\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _save_model(save_path: Path, model: tf.keras.Model) -> None:\n",
    "        \"\"\"\n",
    "        Saves a given model to the specified path.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            create_directories([save_path.parent])\n",
    "            model.save(save_path)\n",
    "            logger.info(f\"Model saved at: {save_path}\")\n",
    "        \n",
    "        except Exception as exception_error:\n",
    "            logger.error(f\"Unexpected error while saving the model at {save_path}: {exception_error}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09059d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    config_manager = ConfigurationManager()\n",
    "    training_config = config_manager.get_training_config()\n",
    "\n",
    "    training_constructor = ModelTraining(config=training_config)\n",
    "    training_constructor.get_base_model()\n",
    "    training_constructor.get_data_generators()\n",
    "    training_constructor.train()\n",
    "    training_constructor.save_class_indices()\n",
    "    training_constructor.resume_train(add_epochs=1)\n",
    "\n",
    "except Exception as exception_error:\n",
    "    logger.exception(f\"Unexpected error during model training pipeline: {exception_error}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727a36cc",
   "metadata": {},
   "source": [
    "## Version 2 - With custom callback using Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3066b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import dagshub\n",
    "import mlflow\n",
    "\n",
    "from math import ceil\n",
    "from typing import Union\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, DirectoryIterator # type: ignore\n",
    "from tensorflow.keras.callbacks import Callback # type: ignore\n",
    "from pathlib import Path\n",
    "from dataclasses import asdict\n",
    "from datetime import datetime\n",
    "\n",
    "from cnnClassifier.utils.common import create_directories, save_json, save_tf_model, convert_paths_to_str\n",
    "from cnnClassifier import get_logger\n",
    "\n",
    "# Initializing the logger\n",
    "logger = get_logger()\n",
    "\n",
    "class CheckpointCallback(Callback):\n",
    "    \"\"\"\n",
    "    A custom Keras callback that saves the model whenever the validation accuracy improves.\n",
    "    The model file name includes the epoch number, training accuracy, and validation accuracy.\n",
    "\n",
    "    Attributes:\n",
    "        save_directory (Path): Directory where the model should be saved.\n",
    "        model (tf.keras.Model): Reference to the model being trained.\n",
    "        best_val_acc (float): Tracks the best validation accuracy seen so far.\n",
    "    \"\"\"\n",
    "    def __init__(self, save_directory: Path, model_to_save: tf.keras.Model) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the callback with directory and model reference.\n",
    "\n",
    "        Args:\n",
    "            save_directory (Path): Where to save the best model checkpoints.\n",
    "            model (tf.keras.Model): The model being trained (required for manual saving).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.best_val_accuracy = 0\n",
    "        self.save_directory = save_directory\n",
    "        self.model_to_save = model_to_save\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch: int, logs: dict[str, float] = None) -> None:\n",
    "        \"\"\"\n",
    "        Called at the end of each epoch. Checks validation accuracy and saves model if improved.\n",
    "\n",
    "        Args:\n",
    "            epoch (int): The index of the current epoch (0-based).\n",
    "            logs (dict[str, float]): Dictionary containing metrics like accuracy, val_accuracy, etc.\n",
    "        \"\"\"\n",
    "        # Validating logs is available\n",
    "        if not logs:\n",
    "            logger.warning(f\"Logs not found. Skipping model save.\")\n",
    "            return\n",
    "\n",
    "        # Getting metrics from logs\n",
    "        val_acc = logs.get(\"val_accuracy\")\n",
    "        train_acc = logs.get(\"accuracy\")\n",
    "\n",
    "        # If validation accuracy isn't available, skip saving\n",
    "        if val_acc is None:\n",
    "            logger.warning(\"val_accuracy not found in logs. Skipping model save.\")\n",
    "            return\n",
    "\n",
    "        # If current model is better than prior best model, saving the model\n",
    "        if val_acc > self.best_val_accuracy:\n",
    "            self.best_val_accuracy = val_acc\n",
    "\n",
    "            # Construct model filename with padded epoch, train_acc, and val_acc\n",
    "            model_path = Path(self.save_directory / f\"model_e{epoch+1:02d}_acc{train_acc:.4f}_vacc{val_acc:.4f}.h5\")\n",
    "\n",
    "            # Save the model to disk\n",
    "            save_tf_model(save_path=model_path, model=self.model_to_save)\n",
    "            logger.info(f\"Saved new best model at {model_path}\")\n",
    "\n",
    "\n",
    "class MLflowCallback(Callback):\n",
    "    \"\"\"\n",
    "    A custom Keras callback that saves the model whenever the validation accuracy improves.\n",
    "    The model file name includes the epoch number, training accuracy, and validation accuracy.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Optional[dict] = None, checkpoint_path: Path = None) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the callback with directory and model reference.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "\n",
    "\n",
    "    def on_train_begin(self, logs: Optional[dict] = None) -> None:\n",
    "        \"\"\"\n",
    "        Called at the beginning of training.\n",
    "        Logs all the parameters to MLflow.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not mlflow.active_run():\n",
    "                logger.error(f\"Unable to find an active MLFlow run.\")\n",
    "                raise ValueError(f\"Unable to find an active MLFlow run.\")\n",
    "\n",
    "            # Flattening config as MLFlow only accepts ints, strs, floats, and such\n",
    "            config_dict = convert_paths_to_str(asdict(self.config))\n",
    "            flatten_config_dict = {}\n",
    "\n",
    "            for key, value in config_dict.items():\n",
    "                if isinstance(value, dict):\n",
    "                    for sub_key, sub_value in value.items():\n",
    "                        flatten_config_dict[f\"{key}.{sub_key}\"] = sub_value\n",
    "                elif isinstance(value, list):\n",
    "                    flatten_config_dict[key] = str(value)\n",
    "                else:\n",
    "                    flatten_config_dict[key] = value\n",
    "\n",
    "            for key, value in flatten_config_dict.items():\n",
    "                mlflow.log_param(key, value)\n",
    "\n",
    "            # Saving checkpoint path\n",
    "            mlflow.log_param(\"Checkpoint Path\", str(self.checkpoint_path))\n",
    "\n",
    "        except Exception as exception_error:\n",
    "            logger.error(f\"Unexpected error while logging params in MLflow: {exception_error}\")\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch: int, logs: dict[str, float] = None) -> None:\n",
    "        \"\"\"\n",
    "        Called at the end of each epoch.\n",
    "        Logs the accuracy metrics and saves the model if val_accuracy improves.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not logs:\n",
    "                logger.warning(f\"Logs not found. Skipping model save to MLflow.\")\n",
    "                return\n",
    "            \n",
    "            # Registering epoch id as a metric\n",
    "            mlflow.log_metric(\"epoch\", epoch + 1, step=epoch)\n",
    "\n",
    "            # Logging all metrics\n",
    "            for key, value in logs.items():\n",
    "                mlflow.log_metric(key, value, step=epoch)\n",
    "\n",
    "        except Exception as exception_error:\n",
    "            logger.error(f\"Unexpected error while logging metrics in MLflow: {exception_error}\")\n",
    "\n",
    "\n",
    "class ModelTraining:\n",
    "    \"\"\"\n",
    "    Initializes training pipeline with given configuration.\n",
    "\n",
    "    Core Responsibilities:\n",
    "    - Load a pre-defined base model from disk and recompile it with a fresh optimizer.\n",
    "    - Set up data generators for training and validation, with optional augmentation.\n",
    "    - Train the model across multiple epochs with optional checkpointing.\n",
    "    - Resume training from where it left off.\n",
    "    - Save class label mappings and model artifacts.\n",
    "\n",
    "    Public Methods:\n",
    "    - get_base_model(): Load and compile the pre-trained base model.\n",
    "    - get_data_generators(): Prepare train and validation data generators.\n",
    "    - train(): Train the model with checkpointing on best validation accuracy.\n",
    "    - resume_train(add_epochs): Continue training the model for additional epochs.\n",
    "    - save_class_indices(): Save class-to-index mapping as JSON for reproducibility.\n",
    "\n",
    "    Private Utilities:\n",
    "    - _build_generator(): Helper to construct data generators with standard settings.\n",
    "    - _get_callbacks(): Constructs and returns a list of Keras-compatible callbacks based on config settings.\n",
    "    - _create_checkpoint(): Creates a checkpoint directory and stores training metadata.\n",
    "    - _get_optimizer(): Returns optimizer based on config.\n",
    "    - _count_images_in_directory(): Utility to count image files recursively.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: ModelTrainingConfig) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the model training pipeline.\n",
    "\n",
    "        - Sets random seeds for reproducibility.\n",
    "        - Prepares internal attributes for managing training, checkpoints, and model state.\n",
    "        \"\"\"\n",
    "        # Store configuration\n",
    "        self.config = config\n",
    "\n",
    "        # Set random seeds\n",
    "        seed = self.config.params_seed if hasattr(self.config, \"params_seed\") else 1234\n",
    "        tf.random.set_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "        # Initialize model and training attributes\n",
    "        self.output_model = None\n",
    "        self.training_generator = None\n",
    "        self.valid_generator = None\n",
    "        self.training_images = None\n",
    "        self.validation_images = None\n",
    "        self.last_epoch = 0\n",
    "        self.additional_epochs = 0\n",
    "        self.best_val_accuracy = 0\n",
    "\n",
    "        # Initialize checkpoint directory path with timestamp\n",
    "        self.curr_time = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "        self.checkpoint_path = Path(self.config.root_dir / f\"Checkpoint_{self.curr_time}\")\n",
    "\n",
    "\n",
    "    def __enter__(self) -> \"ModelTraining\":\n",
    "        \"\"\"\n",
    "        Called when entering the 'with' block.\n",
    "        Starts an MLflow run if enabled in config.\n",
    "\n",
    "        Returns:\n",
    "        - self: The instance of ModelTraining.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if mlflow.active_run():\n",
    "                mlflow.end_run()\n",
    "\n",
    "            if self.config.params_mlflow:\n",
    "                # Start a named MLflow run\n",
    "                mlflow.start_run()\n",
    "                mlflow.set_tag(\"mlflow.runName\", f\"Run_{str(self.curr_time)}\")\n",
    "                logger.info(f\"Initializing MLflow run\")\n",
    "\n",
    "        except Exception as exception_error:\n",
    "            logger.error(f\"Unexpected error while setting up MLFlow: {exception_error}\")\n",
    "            raise\n",
    "        \n",
    "        return self\n",
    "\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback) -> None:\n",
    "        \"\"\"\n",
    "        Called when exiting the 'with' block.\n",
    "        Ends the MLflow run if active.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if mlflow.active_run():\n",
    "                mlflow.end_run()\n",
    "\n",
    "        except Exception as exception_error:\n",
    "            logger.error(f\"Unexpected error while stopping MLflow: {exception_error}\")\n",
    "            raise\n",
    " \n",
    "\n",
    "    def get_base_model(self) -> None:\n",
    "        \"\"\"\n",
    "        Loads the base model form specified path.\n",
    "        \"\"\"\n",
    "        model_path = Path(self.config.updated_base_model)\n",
    "\n",
    "        if not model_path.exists():\n",
    "            logger.error(f\"Could not find model at {model_path}. Run the Base Model pipeline stage first.\")\n",
    "            raise FileNotFoundError(f\"Could not find model at {model_path}. Run the Base Model pipeline stage first.\")\n",
    "        \n",
    "        try:\n",
    "            self.output_model = tf.keras.models.load_model(model_path)\n",
    "            logger.info(f\"Successfully loaded the base model from {model_path}.\")\n",
    "\n",
    "            # Enabling Eager Execution (optional in TF 2.x) due to library requirement\n",
    "            tf.config.run_functions_eagerly(True)\n",
    "\n",
    "            # Recompile model with a fresh optimizer (required after loading)\n",
    "            self.output_model.compile(\n",
    "                optimizer=self._get_optimizer(),\n",
    "                loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                metrics=[\"accuracy\"]\n",
    "            )\n",
    "            logger.info(f\"Successfully recomplied the model.\")\n",
    "\n",
    "        except Exception as exception_error:\n",
    "            logger.error(f\"Unexpected error while loading the update base model at {model_path}: {exception_error}\")\n",
    "            raise \n",
    "            \n",
    "    \n",
    "    def get_data_generators(self) -> None:\n",
    "        \"\"\"\n",
    "        Update train and validation data generators using ImageDataGenerator.\n",
    "        Applies augmentation only on training data if enabled.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"Preparing ImageDataGenerators...\")\n",
    "\n",
    "            train_datagen = ImageDataGenerator(rescale=1.0/255, **self.config.params_if_augmentation)\n",
    "            valid_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "            self.training_generator = self._build_generator(train_datagen, self.config.training_data, \"Train\")\n",
    "            self.valid_generator = self._build_generator(valid_datagen, self.config.validation_data, \"Valid\")\n",
    "\n",
    "            # Ensure class-to-index mapping is consistent\n",
    "            if self.training_generator.class_indices != self.valid_generator.class_indices:\n",
    "                logger.error(\"Mismatch in class indices between train and validation generators!\")\n",
    "                raise ValueError(\"Mismatch in class indices between train and validation generators!\")\n",
    "\n",
    "            logger.info(\"ImageDataGenerators created successfully.\")\n",
    "        \n",
    "        except Exception as exception_error:\n",
    "            logger.error(f\"Unexpected error while creating data generators: {exception_error}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    def train(self) -> None:\n",
    "        \"\"\"\n",
    "        Trains the model using prepared generators.\n",
    "        \"\"\"\n",
    "        if self.output_model == None:\n",
    "            logger.error(\"Base model not found. Run get_base_model() before calling train().\")\n",
    "            raise ValueError(\"Base model not found. Run get_base_model() before calling train().\")\n",
    "        \n",
    "        if (self.config.params_checkpoint) and (not self.checkpoint_path.exists()) and (self.config.params_epochs >= 1):\n",
    "            self._create_checkpoint()\n",
    "\n",
    "        try:\n",
    "            logger.info(\"Initializing model training...\")\n",
    "\n",
    "            # Counting the images in each of the datasets\n",
    "            self.training_images = self._count_images_in_directory(self.config.training_data)\n",
    "            self.validation_images = self._count_images_in_directory(self.config.validation_data)\n",
    "\n",
    "            # Initializing the custom callback from tf.Keras\n",
    "            custom_callback = self._get_callbacks()\n",
    "\n",
    "            # Fitting the model\n",
    "            total_epochs = self.config.params_epochs + self.additional_epochs\n",
    "\n",
    "            self.output_model.fit(\n",
    "                self.training_generator,\n",
    "                validation_data=self.valid_generator,\n",
    "                initial_epoch=self.last_epoch,         # Sets starting point for correct logging\n",
    "                epochs=total_epochs,                        \n",
    "                steps_per_epoch=ceil(self.training_images / self.config.params_batch_size),\n",
    "                validation_steps=ceil(self.validation_images / self.config.params_batch_size),\n",
    "                callbacks=[custom_callback],\n",
    "                verbose=1\n",
    "            )\n",
    "    \n",
    "            # Updating number of epochs completed for resume train\n",
    "            self.last_epoch = total_epochs\n",
    "\n",
    "            logger.info(\"Successfully trained model based on provided parameters.\")\n",
    "            save_tf_model(save_path=self.config.trained_model_path, model=self.output_model)\n",
    "\n",
    "        except Exception as exception_error:\n",
    "            logger.error(f\"Unexpected error while training the model: {exception_error}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    def resume_train(self, add_epochs: int) -> None:\n",
    "        \"\"\"\n",
    "        Resumes model training for additional number of epochs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.additional_epochs == None:\n",
    "                self.additional_epochs = 0\n",
    "            self.additional_epochs += add_epochs\n",
    "\n",
    "            self.train()\n",
    "\n",
    "        except Exception as exception_error:\n",
    "            logger.error(f\"Unexpected error while resuming training: {exception_error}\")\n",
    "            raise\n",
    "    \n",
    "\n",
    "    def save_class_indices(self) -> None:\n",
    "        \"\"\"\n",
    "        Saves the class index mapping as a JSON file for future reference.\n",
    "        \"\"\"\n",
    "        if self.training_generator == None:\n",
    "            logger.error(\"Class indices not found. Run get_data_generators() before calling save_class_indices().\")\n",
    "            raise ValueError(\"Class indices not found. Run get_data_generators() before calling save_class_indices().\")\n",
    "\n",
    "        try:\n",
    "            save_path = Path(self.config.root_dir / \"class_indices.json\")\n",
    "            save_json(save_path=save_path, data=self.training_generator.class_indices)\n",
    "\n",
    "            if self.config.params_checkpoint and self.checkpoint_path.exists():\n",
    "                checkpoint_save_path = Path(self.checkpoint_path / \"class_indices.json\")\n",
    "                save_json(save_path=checkpoint_save_path, data=self.training_generator.class_indices)\n",
    "\n",
    "        except Exception as exception_error:\n",
    "            logger.error(f\"Unexpected error while saving class indices: {exception_error}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    def _build_generator(self, datagen: ImageDataGenerator, data_path: Union[str, Path], tag: str) -> DirectoryIterator:\n",
    "        \"\"\"\n",
    "        Helper to build a flow_from_directory generator with consistent options.\n",
    "\n",
    "        Args:\n",
    "        - datagen (ImageDataGenerator): Instance of the ImageDataGenerator.\n",
    "        - data_path (Union[str, Path]): Path to the directory containing images.\n",
    "        - tag (str): Label for logging context (\"Train\" or \"Valid\").\n",
    "\n",
    "        Returns:\n",
    "        - DirectoryIterator: Configured Keras generator for the given directory.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            data_path = Path(data_path)\n",
    "\n",
    "            if not data_path.exists():\n",
    "                logger.error(f\"{tag.title()} directory not found: {data_path}\")\n",
    "                raise FileNotFoundError(f\"{tag.title()} directory not found: {data_path}\")\n",
    "\n",
    "            # Building generator\n",
    "            generator_unit = datagen.flow_from_directory(\n",
    "                directory=data_path,\n",
    "                target_size=self.config.params_image_size[:2],\n",
    "                batch_size=self.config.params_batch_size,\n",
    "                class_mode=\"categorical\",\n",
    "                shuffle=True,\n",
    "            )\n",
    "\n",
    "            return generator_unit\n",
    "\n",
    "        except Exception as exception_error:\n",
    "            logger.error(f\"Unexpected error while build generator: {exception_error}\")\n",
    "            raise\n",
    "    \n",
    "\n",
    "    def _get_callbacks(self) -> list[Callback]:\n",
    "        \"\"\"\n",
    "        Constructs and returns a list of Keras-compatible callbacks based on config settings.\n",
    "\n",
    "        Returns:\n",
    "            List[Callback]: A list of callbacks to be used during model training.\n",
    "\n",
    "        Handles:\n",
    "        - Checkpointing the model if 'params_checkpoint' is True.\n",
    "        - Logging to MLflow via DagsHub if 'params_mlflow' is True.\n",
    "        \"\"\"\n",
    "        custom_callback = []\n",
    "\n",
    "        # Add checkpoint callback if enabled in config\n",
    "        try:\n",
    "            if self.config.params_checkpoint:\n",
    "                custom_callback.append(CheckpointCallback(save_directory=self.checkpoint_path, model_to_save=self.output_model))\n",
    "        \n",
    "        except Exception as exception_error:\n",
    "            logger.error(f\"Unexpected error while loading the checkpoint callback: {exception_error}\")\n",
    "\n",
    "        # Add MLflow callback if enabled in config\n",
    "        try:\n",
    "            if self.config.params_mlflow:\n",
    "                dagshub.init(repo_owner=\"Vish501\", repo_name=\"TumorTracer\", mlflow=True)\n",
    "                custom_callback.append(MLflowCallback(config=self.config, checkpoint_path=self.checkpoint_path))\n",
    "\n",
    "        except Exception as exception_error:\n",
    "            logger.error(f\"Unexpected error while loading the MLflow callback: {exception_error}\")                \n",
    "\n",
    "        return custom_callback\n",
    "\n",
    "\n",
    "    def _create_checkpoint(self) -> None:\n",
    "        \"\"\"\n",
    "        Creates a checkpoint directory and saves the training configuration as a JSON file.\n",
    "\n",
    "        Purpose:\n",
    "        - Ensures the checkpoint directory exists.\n",
    "        - Saves the current training configuration (hyperparameters) used in that run.\n",
    "        - Helps with reproducibility and traceability for saved models.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"Creating checkpoint directory...\")\n",
    "\n",
    "            save_path = Path(self.checkpoint_path / \"params_used.json\")\n",
    "            create_directories([self.checkpoint_path])\n",
    "\n",
    "            # Convert all Path objects to str recursively\n",
    "            config_dict = convert_paths_to_str(asdict(self.config))\n",
    "\n",
    "            save_json(save_path=save_path, data=config_dict)\n",
    "            \n",
    "            logger.info(f\"Checkpoint directory created.\")\n",
    "\n",
    "        except Exception as exception_error:\n",
    "            logger.error(f\"Unexpected error while creating checkpoint directroy: {exception_error}\")\n",
    "            raise \n",
    "\n",
    "\n",
    "    def _get_optimizer(self) -> tf.keras.optimizers.Optimizer:\n",
    "        \"\"\"\n",
    "        Dynamically selects and returns a TensorFlow optimizer based on the configuration.\n",
    "\n",
    "        Returns:\n",
    "            tf.keras.optimizers.Optimizer: Configured optimizer instance for model compilation.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Normalize optimizer name to lowercase for consistent matching\n",
    "            optimizer_name = self.config.params_optimizer.strip().upper()\n",
    "            optimizer = None\n",
    "\n",
    "            # Select optimizer based on configuration\n",
    "            if optimizer_name == \"SGD\":\n",
    "                optimizer = tf.keras.optimizers.SGD(learning_rate=self.config.params_learning_rate)\n",
    "\n",
    "            elif optimizer_name == \"RMSPROP\":\n",
    "                optimizer = tf.keras.optimizers.RMSprop(learning_rate=self.config.params_learning_rate) \n",
    "\n",
    "            else:\n",
    "                # Default to Adam if unsupported optimizer name is provided\n",
    "                if optimizer_name != \"ADAM\":\n",
    "                    logger.info(f\"Unsupported optimizer name {optimizer_name} provided. Falling back to 'Adam'.\")\n",
    "                    optimizer_name = \"ADAM\"\n",
    "\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate=self.config.params_learning_rate)\n",
    "\n",
    "            logger.info(f\"Optimizer '{optimizer_name}' initialized and returned.\")\n",
    "            return optimizer\n",
    "        \n",
    "        except Exception as exception_error:\n",
    "            logger.error(f\"Unexpected error while loading optimizer: {exception_error}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _count_images_in_directory(directory_path: Union[str, Path]) -> int:\n",
    "        \"\"\"\n",
    "        Counts the total number of image files in a directory and its subfolders.\n",
    "\n",
    "        Args:\n",
    "        - directory_path (str or Path): Path to the dataset root (e.g., train or valid)\n",
    "\n",
    "        Returns:\n",
    "        - int: Total number of images found\n",
    "        \"\"\"\n",
    "        try:\n",
    "            directory_path = Path(directory_path)\n",
    "            total_images = 0\n",
    "\n",
    "            if not directory_path.exists():\n",
    "                logger.error(f\"Could not find the path {directory_path}\")\n",
    "                raise FileNotFoundError(f\"Could not find the path {directory_path}\")\n",
    "            \n",
    "            for _, _, files in os.walk(directory_path):\n",
    "                total_images += len([f for f in files if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))])\n",
    "\n",
    "            if total_images == 0:\n",
    "                logger.error(f\"No images found in {directory_path}\")\n",
    "                raise ValueError(f\"No images found in {directory_path}\")       \n",
    "\n",
    "            return total_images\n",
    "\n",
    "        except Exception as exception_error:\n",
    "            logger.error(f\"Unexpected error while counting images in directory: {exception_error}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c134e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 07:45:57,929: INFO: common: YAML file: config/config.yaml loaded successfully]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:YAML file: config/config.yaml loaded successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 07:45:57,942: INFO: common: YAML file: params/params.yaml loaded successfully]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:YAML file: params/params.yaml loaded successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 07:45:57,945: INFO: 234186428: Loading configuration from config/config.yaml and parameters from params/params.yaml]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Loading configuration from config/config.yaml and parameters from params/params.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 07:45:57,948: INFO: common: Directory: artifacts created successfully.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Directory: artifacts created successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 07:45:57,951: INFO: common: Directory: artifacts/model_training created successfully.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Directory: artifacts/model_training created successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 07:45:57,952: INFO: 234186428: ModelTrainingConfig created with: ModelTrainingConfig(root_dir=PosixPath('artifacts/model_training'), trained_model_path=PosixPath('artifacts/model_training/trained_model.h5'), updated_base_model=PosixPath('artifacts/base_model/updated_base_model.h5'), training_data=PosixPath('artifacts/data_ingestion/Data/train'), validation_data=PosixPath('artifacts/data_ingestion/Data/valid'), params_augmentation=True, params_checkpoint=True, params_mlflow=True, params_image_size=(224, 224, 3), params_batch_size=16, params_epochs=2, params_optimizer='Adam', params_learning_rate=0.01, params_if_augmentation={'rotation_range': 15, 'width_shift_range': 0.1, 'height_shift_range': 0.1, 'shear_range': 0.1, 'zoom_range': 0.2, 'horizontal_flip': True, 'fill_mode': 'nearest'})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:ModelTrainingConfig created with: ModelTrainingConfig(root_dir=PosixPath('artifacts/model_training'), trained_model_path=PosixPath('artifacts/model_training/trained_model.h5'), updated_base_model=PosixPath('artifacts/base_model/updated_base_model.h5'), training_data=PosixPath('artifacts/data_ingestion/Data/train'), validation_data=PosixPath('artifacts/data_ingestion/Data/valid'), params_augmentation=True, params_checkpoint=True, params_mlflow=True, params_image_size=(224, 224, 3), params_batch_size=16, params_epochs=2, params_optimizer='Adam', params_learning_rate=0.01, params_if_augmentation={'rotation_range': 15, 'width_shift_range': 0.1, 'height_shift_range': 0.1, 'shear_range': 0.1, 'zoom_range': 0.2, 'horizontal_flip': True, 'fill_mode': 'nearest'})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=4, connect=5, read=4, redirect=5, status=5)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='dagshub.com', port=443): Read timed out. (read timeout=120)\")': /Vish501/TumorTracer.mlflow/api/2.0/mlflow/runs/create\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 07:47:59,010: INFO: 609804233: Initializing MLflow run]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Initializing MLflow run\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 07:47:59,296: INFO: 609804233: Successfully loaded the base model from artifacts/base_model/updated_base_model.h5.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Successfully loaded the base model from artifacts/base_model/updated_base_model.h5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 07:47:59,300: INFO: 609804233: Optimizer 'ADAM' initialized and returned.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Optimizer 'ADAM' initialized and returned.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 07:47:59,302: INFO: 609804233: Successfully recomplied the model.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Successfully recomplied the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 07:47:59,304: INFO: 609804233: Preparing ImageDataGenerators...]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Preparing ImageDataGenerators...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 613 images belonging to 4 classes.\n",
      "Found 72 images belonging to 4 classes.\n",
      "[2025-07-07 07:47:59,324: INFO: 609804233: ImageDataGenerators created successfully.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:ImageDataGenerators created successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 07:47:59,325: INFO: 609804233: Creating checkpoint directory...]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Creating checkpoint directory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 07:47:59,326: INFO: common: Directory: artifacts/model_training/Checkpoint_20250707_0745 created successfully.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Directory: artifacts/model_training/Checkpoint_20250707_0745 created successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 07:47:59,328: INFO: common: Directory: artifacts/model_training/Checkpoint_20250707_0745 created successfully.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Directory: artifacts/model_training/Checkpoint_20250707_0745 created successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 07:47:59,329: INFO: common: JSON file saved at: artifacts/model_training/Checkpoint_20250707_0745/params_used.json]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:JSON file saved at: artifacts/model_training/Checkpoint_20250707_0745/params_used.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 07:47:59,330: INFO: 609804233: Checkpoint directory created.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Checkpoint directory created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 07:47:59,332: INFO: 609804233: Initializing model training...]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Initializing model training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"Vish501/TumorTracer\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"Vish501/TumorTracer\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository Vish501/TumorTracer initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository Vish501/TumorTracer initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "/home/codespace/.python/current/lib/python3.12/site-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20s/step - accuracy: 0.3534 - loss: 15.6726 [2025-07-07 08:01:36,494: INFO: common: Directory: artifacts/model_training/Checkpoint_20250707_0745 created successfully.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Directory: artifacts/model_training/Checkpoint_20250707_0745 created successfully.\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 08:01:36,573: INFO: common: Model saved at: artifacts/model_training/Checkpoint_20250707_0745/model_e01_acc0.4274_vacc0.5139.h5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Model saved at: artifacts/model_training/Checkpoint_20250707_0745/model_e01_acc0.4274_vacc0.5139.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 08:01:36,574: INFO: 609804233: Saved new best model at artifacts/model_training/Checkpoint_20250707_0745/model_e01_acc0.4274_vacc0.5139.h5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Saved new best model at artifacts/model_training/Checkpoint_20250707_0745/model_e01_acc0.4274_vacc0.5139.h5\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=4, connect=5, read=4, redirect=5, status=5)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='dagshub.com', port=443): Read timed out. (read timeout=120)\")': /Vish501/TumorTracer.mlflow/api/2.0/mlflow/runs/log-metric\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m928s\u001b[0m 24s/step - accuracy: 0.3552 - loss: 15.5616 - val_accuracy: 0.5139 - val_loss: 4.6126\n",
      "Epoch 2/2\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20s/step - accuracy: 0.6147 - loss: 4.3166 [2025-07-07 08:16:49,081: INFO: common: Directory: artifacts/model_training/Checkpoint_20250707_0745 created successfully.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Directory: artifacts/model_training/Checkpoint_20250707_0745 created successfully.\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 08:16:49,159: INFO: common: Model saved at: artifacts/model_training/Checkpoint_20250707_0745/model_e02_acc0.6166_vacc0.6667.h5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Model saved at: artifacts/model_training/Checkpoint_20250707_0745/model_e02_acc0.6166_vacc0.6667.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 08:16:49,161: INFO: 609804233: Saved new best model at artifacts/model_training/Checkpoint_20250707_0745/model_e02_acc0.6166_vacc0.6667.h5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Saved new best model at artifacts/model_training/Checkpoint_20250707_0745/model_e02_acc0.6166_vacc0.6667.h5\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=4, connect=5, read=4, redirect=5, status=5)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='dagshub.com', port=443): Read timed out. (read timeout=120)\")': /Vish501/TumorTracer.mlflow/api/2.0/mlflow/runs/log-metric\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m913s\u001b[0m 23s/step - accuracy: 0.6147 - loss: 4.3128 - val_accuracy: 0.6667 - val_loss: 3.5983\n",
      "[2025-07-07 08:18:51,173: INFO: 609804233: Successfully trained model based on provided parameters.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Successfully trained model based on provided parameters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 08:18:51,175: INFO: common: Directory: artifacts/model_training created successfully.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Directory: artifacts/model_training created successfully.\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 08:18:51,264: INFO: common: Model saved at: artifacts/model_training/trained_model.h5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Model saved at: artifacts/model_training/trained_model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 08:18:51,268: INFO: common: Directory: artifacts/model_training created successfully.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Directory: artifacts/model_training created successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 08:18:51,270: INFO: common: JSON file saved at: artifacts/model_training/class_indices.json]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:JSON file saved at: artifacts/model_training/class_indices.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 08:18:51,274: INFO: common: Directory: artifacts/model_training/Checkpoint_20250707_0745 created successfully.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Directory: artifacts/model_training/Checkpoint_20250707_0745 created successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 08:18:51,276: INFO: common: JSON file saved at: artifacts/model_training/Checkpoint_20250707_0745/class_indices.json]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:JSON file saved at: artifacts/model_training/Checkpoint_20250707_0745/class_indices.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 08:18:51,278: INFO: 609804233: Initializing model training...]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Initializing model training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"Vish501/TumorTracer\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"Vish501/TumorTracer\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository Vish501/TumorTracer initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository Vish501/TumorTracer initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/4\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20s/step - accuracy: 0.7370 - loss: 2.1388 [2025-07-07 08:32:18,888: INFO: common: Directory: artifacts/model_training/Checkpoint_20250707_0745 created successfully.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Directory: artifacts/model_training/Checkpoint_20250707_0745 created successfully.\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 08:32:18,974: INFO: common: Model saved at: artifacts/model_training/Checkpoint_20250707_0745/model_e03_acc0.7455_vacc0.7500.h5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Model saved at: artifacts/model_training/Checkpoint_20250707_0745/model_e03_acc0.7455_vacc0.7500.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 08:32:18,975: INFO: 609804233: Saved new best model at artifacts/model_training/Checkpoint_20250707_0745/model_e03_acc0.7455_vacc0.7500.h5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Saved new best model at artifacts/model_training/Checkpoint_20250707_0745/model_e03_acc0.7455_vacc0.7500.h5\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=4, connect=5, read=4, redirect=5, status=5)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='dagshub.com', port=443): Read timed out. (read timeout=120)\")': /Vish501/TumorTracer.mlflow/api/2.0/mlflow/runs/log-metric\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m916s\u001b[0m 24s/step - accuracy: 0.7373 - loss: 2.1401 - val_accuracy: 0.7500 - val_loss: 2.0348\n",
      "Epoch 4/4\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20s/step - accuracy: 0.7838 - loss: 1.8424 [2025-07-07 08:47:31,022: INFO: common: Directory: artifacts/model_training/Checkpoint_20250707_0745 created successfully.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Directory: artifacts/model_training/Checkpoint_20250707_0745 created successfully.\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 08:47:31,108: INFO: common: Model saved at: artifacts/model_training/Checkpoint_20250707_0745/model_e04_acc0.7537_vacc0.7917.h5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Model saved at: artifacts/model_training/Checkpoint_20250707_0745/model_e04_acc0.7537_vacc0.7917.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 08:47:31,110: INFO: 609804233: Saved new best model at artifacts/model_training/Checkpoint_20250707_0745/model_e04_acc0.7537_vacc0.7917.h5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Saved new best model at artifacts/model_training/Checkpoint_20250707_0745/model_e04_acc0.7537_vacc0.7917.h5\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=4, connect=5, read=4, redirect=5, status=5)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='dagshub.com', port=443): Read timed out. (read timeout=120)\")': /Vish501/TumorTracer.mlflow/api/2.0/mlflow/runs/log-metric\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m912s\u001b[0m 24s/step - accuracy: 0.7831 - loss: 1.8498 - val_accuracy: 0.7917 - val_loss: 1.8100\n",
      "[2025-07-07 08:49:33,245: INFO: 609804233: Successfully trained model based on provided parameters.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Successfully trained model based on provided parameters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 08:49:33,246: INFO: common: Directory: artifacts/model_training created successfully.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Directory: artifacts/model_training created successfully.\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 08:49:33,371: INFO: common: Model saved at: artifacts/model_training/trained_model.h5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cnnClassifierLogger_running:Model saved at: artifacts/model_training/trained_model.h5\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config_manager = ConfigurationManager()\n",
    "    training_config = config_manager.get_training_config()\n",
    "\n",
    "    with ModelTraining(config=training_config) as training_constructor:\n",
    "        training_constructor.get_base_model()\n",
    "        training_constructor.get_data_generators()\n",
    "        training_constructor.train()\n",
    "        training_constructor.save_class_indices()\n",
    "        training_constructor.resume_train(add_epochs=2)\n",
    "\n",
    "except Exception as exception_error:\n",
    "    logger.exception(f\"Unexpected error during model training pipeline: {exception_error}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9432ddc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
